{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BZOO_ELa0PRM",
        "JY04vJjxCGaD",
        "CEqep8tYPLfN",
        "ET2xAQ2Lb9Cs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook introduces EdnaML, the framework we will use to run most of our code. EdnaML's primary use-case is experiment reproducibility.\n",
        "\n",
        "EdnaML defines 2 pipeline abstractions:\n",
        "\n",
        "1. **Experiment Execution**: Here, an ML model is trained on some training data, and evaluated on some corresponding test data. We perform experiment executions with `ednaml.core.EdnaML`\n",
        "2. **Model Deployment**: Here, a trained ML model is used for some predefined task, such as unseen data labeling, supervision, or as a hosted service. We perform deployments with `ednaml.core.EdnaDeploy`\n",
        "\n",
        "\n",
        "EdnaML formalizes the ML experiment and deployment pipelines into concrete steps:\n",
        "\n",
        "1. Configuration: every EdnaML experiment or deployment is managed by a configuration file, with options for each stage. While this can make configuration files somewhat daunting, they are instrumental in ensuring experiment reproducibility. Further, compared to the ad-hox approach of managing experiments by changing variables, configuration files force more modular and extensible design by adhering to EdnaML's core formalisms.\n",
        "2. Data crawling: Experiments and deployments require data. A Crawler, given some URL or folder location, builds a list of all training and testing samples. This is useful in cases where the data might not fit into memory. Then, we can use this list to determine which samples to select for a training batch. Crawlers inherit from `ednaml.crawlers.Crawler`\n",
        "3. Data processing and batching: Raw data needs to be processed so that ML models can use and transform them to predictions. Data from a crawler is passed through a data generator that implements all relevant functionality for preprocessing or live processing, selecting samples for batching, and yielding these batches when requested. Generators inherit from `ednaml.generators.ImageGenerator` or `ednaml.generators.TextGenerator`. Fully bespoke generators should inherit from `ednaml.generators.Generator`. \n",
        "4. Model building: ML Models inherit from `ednaml.models.ModelAbstract`, which formalizes the core elements of an ML model: a forward propagation step, a saving step, and a loading step, among others. \n",
        "5. Training & evaluation: Once a model is constructed, a trainer uses batches from a Generator to train a model, evaluate it, and save it at checkpoints. Trainers inherit from `ednaml.trainers.BaseTrainer`\n",
        "6. Model deployment: A fully trained model can then be deployed with a new set of crawlers and generators to provide predictions on some unseen data. Deployments inherit from `ednaml.deployments.BaseDeploy`.\n",
        "7. Trained model augmentation: Finally, a trained model may need additional functionality not integrated into the base model. For example, a model may need to provide explanations of its predictions, or confidence metrics that were not implemented during training. For these cases, model plugins add functionality to a trained model, such as augmenting outputs, adjusting predictions directly, abstention/rejection, or even active learning. Plugins inherit from `ednaml.plugins.ModelPlugin`\n",
        "\n",
        "## This Notebook\n",
        "\n",
        "Here, we introduce some core EdnaML functionality through several interactive experiments and code examples. While the first few examples are not meant to be changed, the latter examples allow you to change parameters to see changes for yourselves!\n",
        "\n"
      ],
      "metadata": {
        "id": "3LtQmhmQ1T_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Steps\n",
        "\n",
        "We can install either from source or from PyPi. The appropriate option can be selected from the first cell below.\n",
        "\n",
        "**Very Important**. Due to the way Colab installs certain packages, you will need to restart the runtime after installing EdnaML. Then you can proceed with future steps."
      ],
      "metadata": {
        "id": "BZOO_ELa0PRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "9n75eBgQXl7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "install_from = \"source\" # source | pypi\n",
        "branch = \"master\"           # DO NOT CHANGE THIS unless you know what you are doing\n",
        "version = \"0.1.5\"           # DO NOT CHANGE THIS unless you know what you are doing"
      ],
      "metadata": {
        "id": "J7AZehvZ0i2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5j3WfN0fpIT"
      },
      "source": [
        "###  Installation steps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if install_from == \"source\":\n",
        "  ! rm -rf -- GLAMOR ||:\n",
        "  ! git clone -b $branch https://github.com/asuprem/GLAMOR\n",
        "  ! pip install -e GLAMOR/\n",
        "else:\n",
        "  ! python -V\n",
        "  ! pip3 install --pre ednaml==$version"
      ],
      "metadata": {
        "id": "iBWvd4b10spk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restart Runtime here!!!"
      ],
      "metadata": {
        "id": "XHeFF11MXecV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Experiments"
      ],
      "metadata": {
        "id": "JY04vJjxCGaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Running MNIST on EdnaML\n",
        "\n",
        "To introduce most of EdnaML's functionality, we run Resnet-18 on MNIST.\n",
        "\n",
        "MNIST is a classic ML dataset of handwritten digits, [with this Wikipedia article providing additional details](https://en.wikipedia.org/wiki/MNIST_database). MNIST is a basic classification task: build a model that, given a handwritten digit, can tell us what it is, from 0 through 9.\n",
        "\n",
        "Resnet-18 is a fairly standard CNN architecture. CNNs (convolutional neural networks) are well-designed to work with images, because they somewhat replicate how biological eyes percive images. See [introductory notes on CNNs here](https://developer.ibm.com/articles/introduction-to-convolutional-neural-networks/). Resnet-18 is a relatively small model (by modern standards) that performs well on a variety of image classification tasks once it has been trained. See [additional details on Resnet-18 here if you're feeling adventurous](https://github.com/christianversloot/machine-learning-articles/blob/main/resnet-a-simple-introduction.md)."
      ],
      "metadata": {
        "id": "QzffsEaj1P2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, ednaml\n",
        "from ednaml.core import EdnaML\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "hgqvDUeTUK6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have imported our modules, we will initialize an EdnaML experiment from the MNIST configuration. You can find the full configuration, with comments, [here](https://github.com/asuprem/GLAMOR/blob/master/usage-docs/sample-configs/basics/cnn/mnist.yml). You should read through it!\n",
        "\n",
        "We will run it for 5 epochs. This might be slow for CPUs, but fast for GPUs. You can use Colab's free GPU for this!"
      ],
      "metadata": {
        "id": "h9YBiVku90LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = \"./GLAMOR/usage-docs/sample-configs/basics/cnn/mnist.yml\"\n",
        "eml = EdnaML(config=cfg)\n",
        "eml.apply()"
      ],
      "metadata": {
        "id": "fOcH4qxl5sTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eml.train()"
      ],
      "metadata": {
        "id": "IoxeQVS6USRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eml.eval()"
      ],
      "metadata": {
        "id": "-ff_dn4oUSPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Running CIFAR on EdnaML\n",
        "\n",
        "Now we will run Resnet-18 on CIFAR, another classic dataset that is more difficult to work with!\n",
        "\n",
        "CIFAR is another classic ML dataset of thumbnails, [with this Wikipedia article providing additional details](https://www.cs.toronto.edu/~kriz/cifar.html). CIFAR also has 10 classes (there is a CIFAR-100 variant with 100 classes)."
      ],
      "metadata": {
        "id": "MBJsh5aTBzM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, ednaml\n",
        "from ednaml.core import EdnaML\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "Wfb4jIYmBzM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have imported our modules, we will initialize an EdnaML experiment from the CIFAR configuration. You can find the full configuration, with comments, [here](https://github.com/asuprem/GLAMOR/blob/master/usage-docs/sample-configs/basics/cnn/cifar.yml). You should read through it and compare it to the MNIST configuration. \n",
        "\n",
        "Compare \n",
        "\n",
        "We will run it for 5 epochs. This might be slow for CPUs, but fast for GPUs. You can use Colab's free GPU for this!"
      ],
      "metadata": {
        "id": "U21Ph1euBzM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = \"./GLAMOR/usage-docs/sample-configs/basics/cnn/cifar.yml\"\n",
        "eml = EdnaML(config=cfg)\n",
        "eml.apply()"
      ],
      "metadata": {
        "id": "csyb77GfBzM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eml.train()"
      ],
      "metadata": {
        "id": "q54b-SJwBzM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eml.eval()"
      ],
      "metadata": {
        "id": "QtEo1X2zBzM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adjusting Parameters in Configuration Files\n",
        "\n",
        "Now we will adjust the prior experiments by changing some of the configuration details. Ideally, these should be changed by creating a new configuration file and feeding it directly into EdnaML. \n",
        "\n",
        "For interactivity, we will change parameters here!\n",
        "\n",
        "1. But first, a note on configuration files. A complete configuration sample, will all parameters and their respective default values [is provided here](https://github.com/asuprem/GLAMOR/blob/master/usage-docs/config-full.yml). **NOTE**: The exceptions are the LOSS field and OPTIMIZER field, both of whose default values are empty; we have provided values in the above sample to given an example of a simple LOSS and OPTIMIZER structure.\n",
        "\n",
        "2. Second, configurations are in YAML format. To this, we add one best-practice for EdnaML: built in keys should always be in ALL CAPS, while custom keys, such as arguments for functions and classes, should be in lowercase. For example, in the snippet from the full configuration below:\n",
        "\n",
        "```\n",
        "# EXECUTION manages ML training and evaluation. Use with EdnaML\n",
        "EXECUTION:\n",
        "  # A trainer to use, from ednaml.trainers. A custom trainer can be implicitly added\n",
        "  TRAINER: BaseTrainer\n",
        "  # Arguments for the trainer\n",
        "  TRAINER_ARGS: \n",
        "    accumulation_steps: 1\n",
        "```\n",
        "\n",
        "EXECUTION is a built-in top level key. TRAINER and TRAINER_ARGS are also  built-in keys. However, `accumulation_steps` is an argument specifically when the TRAINER is `BaseTrainer`. A different training might not even use the `accumulation_steps` argument. So, it is a custom key that should be in lowercase."
      ],
      "metadata": {
        "id": "CEqep8tYPLfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. MNIST: Adjusting parameters"
      ],
      "metadata": {
        "id": "zgZqppsyRD2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, ednaml\n",
        "from ednaml.core import EdnaML\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "weyU0mICPK5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will change the built-in keys with the `config_inject` argument for EdnaML.\n",
        "\n",
        "We will change the custom keys afterwards."
      ],
      "metadata": {
        "id": "bZ8LBrAYRwBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = \"./GLAMOR/usage-docs/sample-configs/basics/cnn/mnist.yml\"\n",
        "eml = EdnaML(config=cfg, config_inject = [\n",
        "    (\"EXECUTION.EPOCHS\", 2),          # Here, we change the EPOCHS parameter inside the EXECUTION key to be 2 epochs\n",
        "    (\"SAVE.SAVE_FREQUENCY\", 1),       # We adjust SAVE_FREQUENCY from 5 to 1, so that we can save every epoch\n",
        "    (\"SAVE.MODEL_VERSION\", 2),        # We also change the version for this new experiment, so that we do not overwrite an existing model!\n",
        "    (\"LOGGING.STEP_VERBOSE\", 50)      # We adjust how often intermediate results are printed\n",
        "])\n",
        "\n",
        "\n",
        "eml.cfg.OPTIMIZER[0].BASE_LR = 1e-4   # Since the OPTIMIZER parameters are in a list, it is easier to change them outside of `config_inject`\n",
        "\n",
        "eml.cfg.EXECUTION.TRAINER_ARGS[\"accumulation_steps\"] = 1  # We change accumulation steps to 1\n",
        "eml.cfg.SCHEDULER[0].LR_KWARGS[\"step_size\"] = 1           # We adjust the scheduler step size to change every 1 epoch, instead of 5 epochs\n",
        "\n",
        "\n",
        "eml.apply()"
      ],
      "metadata": {
        "id": "q7OWs8YgRKew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eml.train()"
      ],
      "metadata": {
        "id": "pkyaocG_RKex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eml.eval()"
      ],
      "metadata": {
        "id": "mOsqDNK9RKex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Custom Models\n",
        "\n",
        "Now we will look at an example for running a custom model on MNIST, instead of Resnet-18\n",
        "\n",
        "Specifically, we will implement the model from [this medium article](https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118) inside ModelAbstract."
      ],
      "metadata": {
        "id": "Hvqd9xjISki3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, ednaml\n",
        "from ednaml.core import EdnaML\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "MntdPYHbWBdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we define our custom model class\n",
        "from ednaml.models import ModelAbstract\n",
        "from torch import nn\n",
        "import ednaml.core.decorators as edna\n",
        "\n",
        "class MNISTModel(ModelAbstract):\n",
        "  def model_attributes_setup(self, **kwargs):\n",
        "    pass\n",
        "  def model_setup(self, **kwargs):\n",
        "    self.conv1 = nn.Sequential(         \n",
        "        nn.Conv2d(\n",
        "            in_channels=1,              \n",
        "            out_channels=16,            \n",
        "            kernel_size=5,              \n",
        "            stride=1,                   \n",
        "            padding=2,                  \n",
        "        ),                              \n",
        "        nn.ReLU(),                      \n",
        "        nn.MaxPool2d(kernel_size=2),    \n",
        "    )\n",
        "    self.conv2 = nn.Sequential(         \n",
        "        nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "        nn.ReLU(),                      \n",
        "        nn.MaxPool2d(2),                \n",
        "    )\n",
        "    # fully connected layer, output 10 classes\n",
        "    self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "    \n",
        "  def forward_impl(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "    x = x.view(x.size(0), -1)       \n",
        "    output = self.out(x)\n",
        "    return output, x, []    # A ModelAbstract should return the prediction, the features, and any additional output (i.e. the empty list, because we have no additional outputs)"
      ],
      "metadata": {
        "id": "2oKPgV1-WC97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will change the built-in keys with the `config_inject` argument for EdnaML.\n",
        "\n",
        "We will change the custom keys afterwards."
      ],
      "metadata": {
        "id": "CdGyo1d8WBdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = \"./GLAMOR/usage-docs/sample-configs/basics/cnn/mnist.yml\"\n",
        "eml = EdnaML(config=cfg, config_inject = [\n",
        "    (\"SAVE.MODEL_VERSION\", 3),            # We switch to version 3 for this experiment\n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 256),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [256,1,28,28]),   # We will also fix the input size\n",
        "])\n",
        "\n",
        "\n",
        "eml.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "\n",
        "eml.addModelClass(MNISTModel)\n",
        "eml.apply()"
      ],
      "metadata": {
        "id": "4kEUpdmnWBdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eml.train()"
      ],
      "metadata": {
        "id": "efrlTZRaWBdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eml.eval()"
      ],
      "metadata": {
        "id": "fSjeDZVgWBdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your turn\n",
        "\n",
        "Try repeating the above, but for CIFAR 10.\n",
        "\n",
        "You will need to do the following:\n",
        "\n",
        "1. Define a custom CIFAR10 model. You should use the model defined in Step 2 [in this page](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), with the following changes\n",
        "\n",
        "  a. Put the layer definitions (everything in `__init__` EXCEPT for `super().__init__()` in `model_setup(self, **kwargs)`\n",
        "\n",
        "  b. Remove the `__init__` function; ModelAbstract implements it\n",
        "\n",
        "  c. Rename the `forward` function to `forward_impl`\n",
        "\n",
        "  d. Make sure to return outputs, features, and an empty list, just like MNISTModel\n",
        "\n",
        "  e. For this, you will have to change the last line in `forward_impl` to be `output = self.fc3(x)`. Then you can return `output` *and* `x`, *and* an empty list\n",
        "2. Use the `cifar.yml` configuration, with the appropriate changes:\n",
        "  a. You can inject a `SAVE.MODEL_VERSION` to be version 2\n",
        "3. Plug your custom CIFAR model into EdnaML (with `eml.addModelClass`!)\n",
        "\n",
        "You can increase or decrease the batch size, if you want, or change the learning rate (see how we adjusted parameters for MNIST earlier)\n",
        "\n",
        "Your accuracy should be around 50-60%. Not a great model, by any means, but a good start!"
      ],
      "metadata": {
        "id": "ET2xAQ2Lb9Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, ednaml\n",
        "from ednaml.core import EdnaML\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "OO-59ovLdabh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we define our custom model class\n",
        "from ednaml.models import ModelAbstract\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CIFARModel(ModelAbstract):\n",
        "  def model_attributes_setup(self, **kwargs):\n",
        "    pass\n",
        "  def model_setup(self, **kwargs):\n",
        "    pass\n",
        "  def forward_impl(self, x):\n",
        "    pass"
      ],
      "metadata": {
        "id": "Iab9Il3Edabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will change the built-in keys with the `config_inject` argument for EdnaML.\n",
        "\n",
        "We will change the custom keys afterwards."
      ],
      "metadata": {
        "id": "AAUQXoiydabm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = \"./GLAMOR/usage-docs/sample-configs/basics/cnn/cifar.yml\"\n",
        "eml = EdnaML(config=cfg, config_inject = [\n",
        "    (\"SAVE.MODEL_VERSION\", 2),            # We switch to version 2 for this experiment\n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 32),    # CHANGE IF YOU WANT\n",
        "    (\"LOGGING.INPUT_SIZE\", [32,3,32,32]), # CHANGE IF YOU WANT\n",
        "])\n",
        "\n",
        "\n",
        "eml.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "\n",
        "eml.addModelClass(CIFARModel)\n",
        "eml.apply()"
      ],
      "metadata": {
        "id": "4QDGZ5mVdabm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eml.train()"
      ],
      "metadata": {
        "id": "FyOemsWgdabn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eml.eval()"
      ],
      "metadata": {
        "id": "nwLMNyj_dbAY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}